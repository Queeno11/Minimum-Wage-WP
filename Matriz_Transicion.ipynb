{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Para Bruno:\n",
    "#\n",
    "# Si querés correr esto, vas a tener que tener en cuenta lo siguiente:\n",
    "#\n",
    "# 1) Vas a tener que instalar los siguientes paquetes (vas a cmd y pones, por ejemplo, \n",
    "# para instalar numpy, pip install numpy):\n",
    "#       numpy\n",
    "#       pandas\n",
    "#       matplotlib\n",
    "#       seaborn\n",
    "#       tqdm\n",
    "#       datetime\n",
    "#\n",
    "# 2) Reemplazas path por la ruta donde vas a trabajar.\n",
    "#\n",
    "# 3) En esa carpeta, tenés que tener dos carpetas llamadas \"data\\data_in\" donde esté la MLER\n",
    "#\n",
    "# FIN :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "path = r\"C:\\Users\\Nico\\Documents\\Maestría\\Economia de la Distribución\\Monografía Distribución\"\n",
    "path_data = rf\"{path}\\Data\\RDSample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base completa (la proceso Bruno antes y generó los RDSample_{año}\n",
    "# df = pd.read_csv(rf\"{path}\\Data\\Muestra Longitudinal Empleo Registrado.csv\")\n",
    "\n",
    "# dic con las bases de datos apareadas a cada shock\n",
    "datasets = {\n",
    "    \"200307\":'RDSample_2003.dta',\n",
    "    \"200401\":'RDSample_Enero2004.dta',\n",
    "    \"200409\":'RDSample_2004.dta',\n",
    "    \"200505\":'RDSample_2005.dta',\n",
    "    \"200608\":'RDSample_2006.dta',\n",
    "    \"200708\":'RDSample_2007.dta',\n",
    "    \"200808\":'RDSample_2008.dta',\n",
    "    \"201109\":'RDSample_2011.dta',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_relevantes =   [\n",
    "    'ide_trabajador', 'relacion', 'pondera', 'sexo', 'fnac_anu', 'provi',\n",
    "    'letra', 'tam_emp', 'anio', 'hneg', 'hpos', 'age', 'z', 'treated',\n",
    "    'emp_1', 'emp_2', 'emp_3', 'emp_4', 'emp_5', 'emp_6', 'e1', 'e2', 'e3',\n",
    "    'e4', 'e5', 'e6', 'sector', 'sexb1', 'sexb2', 'ages', 'province1',\n",
    "    'province2', 'province3', 'province4', 'province5', 'province6',\n",
    "    'province7', 'province8', 'province9', 'province10', 'province11',\n",
    "    'province12', 'province13', 'province14', 'province15', 'province16',\n",
    "    'province17', 'province18', 'province19', 'province20', 'province21',\n",
    "    'province22', 'province23', 'province24', 'province25', 'secto1',\n",
    "    'secto2', 'secto3', 'secto4', 'secto5', 'secto6', 'secto7', 'secto8',\n",
    "    'secto9', 'secto10', 'secto11', 'secto12', 'secto13', 'secto14'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lista de SMVM\n",
    "fecha_1 = \"200307\"\n",
    "fecha_2 = \"200401\"\n",
    "fecha_3 = \"200409\"\n",
    "fecha_4 = \"200505\"\n",
    "fecha_5 = \"200608\"\n",
    "fecha_6 = \"200708\"\n",
    "fecha_7 = \"200808\"\n",
    "fecha_8 = \"201109\"\n",
    "\n",
    "fecha_list = [fecha_1,fecha_2,fecha_3,fecha_4,\n",
    "                fecha_5,fecha_6,fecha_7,fecha_8] \n",
    "\n",
    "smvm_1 = 250\n",
    "smvm_2 = 350\n",
    "smvm_3 = 450\n",
    "smvm_4 = 510\n",
    "smvm_5 = 760\n",
    "smvm_6 = 900\n",
    "smvm_7 = 1200\n",
    "smvm_8 = 2300\n",
    "\n",
    "bw_1 = 35.524589\n",
    "bw_2 = 58.957529\n",
    "bw_3 = 20.390511\n",
    "bw_4 = 50.533333\n",
    "bw_5 = 58.053068\n",
    "bw_6 = 45.958942\n",
    "bw_7 = 42.297577\n",
    "bw_8 = 131.07617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smvm': 250,\n",
       " 'smvm_prev': 200,\n",
       " 'bw': 35.524589,\n",
       " 'str_en_t': '200307',\n",
       " 'dt_en_t': datetime.datetime(2003, 7, 1, 0, 0),\n",
       " 'str_next': '200312',\n",
       " 'dt_next': datetime.datetime(2003, 12, 1, 0, 0),\n",
       " 'str_prev': '200302',\n",
       " 'dt_prev': datetime.datetime(2003, 2, 1, 0, 0),\n",
       " 'str_prev6': '200301',\n",
       " 'dt_prev6': datetime.datetime(2003, 1, 1, 0, 0),\n",
       " 'str_prev1': '200306',\n",
       " 'dt_prev1': datetime.datetime(2003, 6, 1, 0, 0),\n",
       " 'str_post6': '200312',\n",
       " 'dt_post6': datetime.datetime(2003, 12, 1, 0, 0)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Genero Metadatos de cada uno de los shocks\n",
    "from itertools import cycle\n",
    "aumentos_dict = {}\n",
    "licycle = cycle(fecha_list)\n",
    "actual = next(licycle)\n",
    "prevelem = '200301'\n",
    "smvm_0 = 200\n",
    "\n",
    "for i in range(0,len(fecha_list)):\n",
    "    str_en_t = actual\n",
    "    dt_en_t = datetime.strptime(str_en_t, '%Y%m')\n",
    "    \n",
    "    dt_prev6 = dt_en_t - relativedelta(months=6)\n",
    "    str_prev6 = dt_prev6.strftime('%Y%m')\n",
    "\n",
    "    dt_prev1 = dt_en_t - relativedelta(months=1)\n",
    "    str_prev1 = dt_prev1.strftime('%Y%m')\n",
    "\n",
    "    dt_post6 = dt_en_t + relativedelta(months=5)\n",
    "    str_post6 = dt_post6.strftime('%Y%m')\n",
    "    \n",
    "    dt_sm_prev = datetime.strptime(prevelem, '%Y%m') + relativedelta(months=+1)\n",
    "    str_sm_prev = datetime.strftime(dt_sm_prev, '%Y%m')\n",
    "\n",
    "    if i < len(fecha_list)-1:\n",
    "        str_next_true = next(licycle)\n",
    "        dt_next = datetime.strptime(str_next_true, '%Y%m') + relativedelta(months=-1)\n",
    "        str_next = datetime.strftime(dt_next, '%Y%m')\n",
    "    else:\n",
    "        dt_next = dt_en_t + relativedelta(months=+12)\n",
    "        str_next = datetime.strftime(dt_next, '%Y%m')\n",
    "\n",
    "    aumentos_dict[str_en_t]= {'smvm':eval('smvm_'+str(i+1)),\n",
    "                              'smvm_prev': eval('smvm_'+str(i)),\n",
    "                              'bw': eval('bw_'+str(i+1)),\n",
    "                              'str_en_t': str_en_t,\n",
    "                              'dt_en_t': dt_en_t,\n",
    "                              'str_next': str_next,\n",
    "                              'dt_next' : dt_next,\n",
    "                              'str_prev': str_sm_prev,\n",
    "                              'dt_prev': dt_sm_prev,\n",
    "                              'str_prev6': str_prev6,\n",
    "                              'dt_prev6': dt_prev6,\n",
    "                              'str_prev1': str_prev1,\n",
    "                              'dt_prev1': dt_prev1,\n",
    "                              'str_post6': str_post6,\n",
    "                              'dt_post6': dt_post6,\n",
    "    }\n",
    "    # Seteo para el prox período\n",
    "    prevelem = str_en_t\n",
    "    actual = str_next_true\n",
    "\n",
    "aumentos_dict['200307']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpia_base(df, aumentos_dict, shock):\n",
    "    ''' Procesa la base de datos eliminando las columnas irrelevantes para el shock,\n",
    "        identificando grupo de control, tratamiento y fuera de la muestra y elminina\n",
    "        observaciones nulas y/o inválidas\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): Muestra Longitudinal de Empleo en formato dataframe.\n",
    "        - aumentos_dict (dict): Diccionario con los parámetros de cada shock.\n",
    "        - shock (str): Shock a analizar. El formato debe ser 'YYYYMM'.\n",
    "        Returns:\n",
    "        - df_temp (pd.DataFrame): Dataframe limpio para un shock determinado.\n",
    "    '''\n",
    "    smvm        = aumentos_dict[shock]['smvm']\n",
    "    smvm_prev   = aumentos_dict[shock]['smvm_prev']\n",
    "    bw          = aumentos_dict[shock]['bw']\n",
    "    str_next    = aumentos_dict[shock]['str_next']\n",
    "    str_prev    = aumentos_dict[shock]['str_prev']\n",
    "\n",
    "    \n",
    "    # Construyo el df\n",
    "    df_temp_main = df.loc[:,('rt' + str_prev):('rt' + str_next)]\n",
    "    df_temp_relevantes = df[cols_relevantes]\n",
    "    df_temp = pd.concat([df_temp_main, df_temp_relevantes],\n",
    "                    axis=1)  # axis=1 para concatenar columnas\n",
    "\n",
    "    # Elimino obs irrelevantes\n",
    "    df_temp = df_temp.dropna(how='all')\n",
    "    # df_temp = df_temp.dropna(subset='rt' + shock)\n",
    "\n",
    "    # Clasifico entre Control y Tratamiento\n",
    "    df_temp.loc[(-bw < df_temp['rt' + shock] - smvm ) &\n",
    "                (df_temp['rt' + shock] - smvm < 0),\n",
    "                'Grupo'] = 'Tratamiento'\n",
    "    df_temp.loc[(0 < df_temp['rt' + shock] - smvm ) &\n",
    "                (df_temp['rt' + shock] - smvm < bw), \n",
    "                'Grupo'] = 'Control'\n",
    "    df_temp.loc[df_temp['rt' + shock] - smvm >= bw,\n",
    "                 'Grupo'] = 'Out Sample'\n",
    "\n",
    "    # Elimino obs que tengan salario en t0 menor al smvm previo\n",
    "    df_temp = df_temp[df_temp['rt' + shock] >= smvm_prev]\n",
    "\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero dfs por cada SMVM y los guardo en el diccionario aumentos_dfs\n",
    "aumentos_dfs = {}\n",
    "for shock in aumentos_dict.keys():\n",
    "\n",
    "    df_temp = pd.read_stata(rf\"{path_data}/{datasets[shock]}\")\n",
    "    # df_temp = limpia_base(df, aumentos_dict, shock) # Ya no lo uso, Bruno me pasó las bases ya limpias\n",
    "    aumentos_dfs[shock] = df_temp\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnas_salariales_con_controles(df, cols_relevantes, first_col, last_col, nan_luego_de_destruccion=False):\n",
    "    ''' Limita el df a las columnas salariales entre dirst:col y last_col, más los controles. '''\n",
    "    \n",
    "    df_temp_main = df.loc[:, first_col: last_col]\n",
    "    \n",
    "    if nan_luego_de_destruccion:\n",
    "    # SOLO COLS DE SALARIO: Si aparece un nan en alguna columna, la columna siguiente es nan. Ya perdió el trabajo!\n",
    "    # https://stackoverflow.com/questions/63026512/pandas-set-all-values-after-first-nan-to-nan\n",
    "\n",
    "        df_temp_main = df_temp_main.where(df_temp_main.notna().cumprod(axis=1).eq(1))\n",
    "\n",
    "    # SOLO COLS DE CONTROLES:\n",
    "    df_temp_relevantes = df[cols_relevantes]\n",
    "\n",
    "    # Concateno\n",
    "    df = pd.concat([df_temp_main, df_temp_relevantes], axis=1) # axis=1 para concatenar columnas\n",
    "\n",
    "    return df\n",
    "\n",
    "def shock_transition(aumentos_dfs, aumentos_dict, shock):\n",
    "    ''' Genera la matriz de transición de un shock dado para el grupo de control y de tratamiento.\n",
    "\n",
    "    Parameters:\n",
    "    - aumentos_dfs (dict): Diccionario con los dataframes de cada shock.\n",
    "    - aumentos_dict (dict): Diccionario con los parámetros de cada shock.\n",
    "    - shock (str): Shock a analizar. El formato debe ser 'YYYYMM'.\n",
    "    Returns:\n",
    "    - df_transicion (pd.DataFrame): Dataframe con la matriz de transición. En las columnas se encuentran los\n",
    "    '''\n",
    "    smvm      = aumentos_dict[shock]['smvm']\n",
    "    dt_en_t   = aumentos_dict[shock]['dt_en_t']\n",
    "    str_en_t   = aumentos_dict[shock]['str_en_t']\n",
    "    str_post6 = aumentos_dict[shock]['str_post6']\n",
    "    str_prev1 = aumentos_dict[shock]['str_prev1']\n",
    "    df_temp   = aumentos_dfs[shock] \n",
    "\n",
    "    # cols_relevantes = ['ide_trabajador', 'relacion', 'pondera', 'sexo',\n",
    "    #                 'fnac_anu', 'provi', 'letra', 'r32', 'r34', 'Grupo', 'rt' + str_prev1]\n",
    "    first_col = str('rt'+str_prev1)\n",
    "    t_col     = str('rt'+str_en_t)\n",
    "    last_col  = str('rt'+str_post6)\n",
    "\n",
    "    # Droppeo las filas que son todas nan\n",
    "    df_temp = df_temp[-df_temp.loc[:, first_col: last_col].isna().all(axis=1)]\n",
    "\n",
    "    # Me quedo solo con las columnas a 6 meses y las cols relevantes\n",
    "    df_mw_match = columnas_salariales_con_controles(df_temp, cols_relevantes, first_col, last_col, nan_luego_de_destruccion=True)\n",
    "    df_all      = columnas_salariales_con_controles(df_temp, cols_relevantes, first_col, last_col, nan_luego_de_destruccion=False)\n",
    "\n",
    "    ### Analisis de la base all \n",
    "    # Analizo si al 6to mes el trabajador tiene o no algún salario declarado mayor al smvm\n",
    "    df_has_job = df_all.groupby('ide_trabajador').agg({last_col:'sum'}).replace(0, np.nan).dropna()\n",
    "    # Creo serie que me dice si al 6to mes el trabajador tuvo un salario mayor al smvm\n",
    "    df_has_job_smvm = df_has_job[df_has_job[last_col]>=smvm] \n",
    "\n",
    "    ### Analisis de la base de solo MW match\n",
    "    ## Clasifico según \"Continua\" o \"No Continua\" en el empleo\n",
    "    # Continua implica que tuvo salario los 6 meses (Si no tiene nans en todos los salarios, entonces continua))\n",
    "    df_mw_match['keeps_job'] = np.where(df_mw_match.loc[:,t_col:last_col].isna().sum(axis=1) == 0, 1, 0)\n",
    "\n",
    "    # Creo columna con el último sueldo de los 6 meses\n",
    "    for i in tqdm(df_mw_match.index):\n",
    "        df_mw_match.loc[i,'last_wage'] = df_mw_match.loc[i, first_col: last_col].dropna().iat[-1]\n",
    "\n",
    "    # Creo columna con la media del sueldo de los 6 meses        \n",
    "    df_mw_match['mean_wage'] = df_mw_match.loc[:, first_col: last_col].mean(axis=1)\n",
    "\n",
    "    # Creo columna de has_job, para ver si la persona tuvo trabajo a los 6 meses\n",
    "    df_mw_match['has_job'] = 0\n",
    "    df_mw_match['has_job_geq_smvm'] = 0\n",
    "    df_mw_match.loc[\n",
    "        df_mw_match.ide_trabajador.isin(df_has_job.index), \n",
    "        'has_job'] = 1\n",
    "    df_mw_match.loc[\n",
    "        df_mw_match.ide_trabajador.isin(df_has_job_smvm.index), \n",
    "        'has_job'] = 1\n",
    "\n",
    "    # Columna de new_job\n",
    "    df_mw_match['new_job'] = np.where(\n",
    "        (df_mw_match['keeps_job']==0) & (df_mw_match['has_job']==1),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Otras columnas\n",
    "    df_mw_match['running_variable'] = df_mw_match['rt' + str_prev1] - smvm\n",
    "    df_mw_match['smvm'] = smvm\n",
    "\n",
    "    # Exporto\n",
    "    assert df_mw_match.shape[0] == aumentos_dfs[shock].shape[0]\n",
    "    df_mw_match.to_stata(f'RDSample_{shock}_processed.dta', write_index=False)\n",
    "    \n",
    "    # Tabla resumen\n",
    "    out_has_job   = pd.crosstab(\n",
    "        df_mw_match['treated'], \n",
    "        df_mw_match['has_job']\n",
    "        ).rename(columns={0:'no_job',1:'has_job'})\n",
    "    out_keeps_job  = pd.crosstab(\n",
    "        df_mw_match['treated'], \n",
    "        df_mw_match['keeps_job']\n",
    "        ).rename(columns={0:'lost_job',1:'keeps_job'})\n",
    "    out_new_job  = pd.crosstab(\n",
    "        df_mw_match['treated'], \n",
    "        df_mw_match['new_job']\n",
    "        ).rename(columns={0:'no_new_job',1:'new_job'})\n",
    "\n",
    "    out = pd.concat([out_has_job, out_keeps_job, out_new_job], axis=1)\n",
    "    out['shock'] = dt_en_t\n",
    "    out.set_index('shock', inplace=True, append=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "def all_shocks_transition_matrix(aumentos_dfs, aumentos_dict):\n",
    "    outs = []\n",
    "    for shock in aumentos_dict.keys():\n",
    "        print(shock)\n",
    "        out_temp = shock_transition(aumentos_dfs, aumentos_dict, shock)\n",
    "        outs += [out_temp]\n",
    "    out = pd.concat(outs, axis=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2794/2794 [00:01<00:00, 1467.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4366/4366 [00:03<00:00, 1240.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2496/2496 [00:02<00:00, 1031.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8525/8525 [00:10<00:00, 799.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8536/8536 [00:20<00:00, 421.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5214/5214 [00:11<00:00, 442.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3991/3991 [00:08<00:00, 479.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6582/6582 [00:06<00:00, 1064.94it/s]\n"
     ]
    }
   ],
   "source": [
    "tabla = all_shocks_transition_matrix(aumentos_dfs, aumentos_dict)\n",
    "tabla.to_excel(\"Matriz_transición.xlsx\")\n",
    "\n",
    "tabla = tabla.reset_index()\n",
    "tabla['shock'] = tabla['shock'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "\n",
    "# for df in aumentos_dfs.values():\n",
    "#     display(df[df.ide_trabajador == 39547340])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shock = '200307'\n",
    "\n",
    "# if 1==1:\n",
    "\n",
    "#     smvm      = aumentos_dict[shock]['smvm']\n",
    "#     dt_en_t   = aumentos_dict[shock]['dt_en_t']\n",
    "#     str_en_t   = aumentos_dict[shock]['str_en_t']\n",
    "#     str_post6 = aumentos_dict[shock]['str_post6']\n",
    "#     str_prev1 = aumentos_dict[shock]['str_prev1']\n",
    "#     df_temp   = aumentos_dfs[shock] \n",
    "\n",
    "#     # cols_relevantes = ['ide_trabajador', 'relacion', 'pondera', 'sexo',\n",
    "#     #                 'fnac_anu', 'provi', 'letra', 'r32', 'r34', 'Grupo', 'rt' + str_prev1]\n",
    "#     first_col = str('rt'+str_prev1)\n",
    "#     t_col     = str('rt'+str_en_t)\n",
    "#     last_col  = str('rt'+str_post6)\n",
    "\n",
    "#     # Droppeo las filas que son todas nan\n",
    "#     df_temp = df_temp[-df_temp.loc[:, first_col: last_col].isna().all(axis=1)]\n",
    "\n",
    "\n",
    "#     # Me quedo solo con las columnas a 6 meses y las cols relevantes\n",
    "#     df_mw_match = columnas_salariales_con_controles(df_temp, cols_relevantes, first_col, last_col, nan_luego_de_destruccion=True)\n",
    "#     df_all      = columnas_salariales_con_controles(df_temp, cols_relevantes, first_col, last_col, nan_luego_de_destruccion=False)\n",
    "\n",
    "#     ### Analisis de la base all \n",
    "#     # Analizo si al 6to mes el trabajador tiene o no algún salario declarado mayor al smvm\n",
    "#     df_has_job = df_all.groupby('ide_trabajador').agg({last_col:'sum'}).replace(0, np.nan).dropna()\n",
    "#     # Creo serie que me dice si al 6to mes el trabajador tuvo un salario mayor al smvm\n",
    "#     df_has_job_smvm = df_has_job[df_has_job[last_col]>=smvm] \n",
    "\n",
    "#     ### Analisis de la base de solo MW match\n",
    "#     ## Clasifico según \"Continua\" o \"No Continua\" en el empleo\n",
    "#     # Continua implica que tuvo salario los 6 meses (Si no tiene nans en todos los salarios, entonces continua))\n",
    "#     df_mw_match['keeps_job'] = np.where(df_mw_match.loc[:,t_col:last_col].isna().sum(axis=1) == 0, 1, 0)\n",
    "\n",
    "#     # Creo columna con el último sueldo de los 6 meses\n",
    "#     for i in tqdm(df_mw_match.index):\n",
    "#         df_mw_match.loc[i,'last_wage'] = df_mw_match.loc[i, first_col: last_col].dropna().iat[-1]\n",
    "\n",
    "#     # Creo columna con la media del sueldo de los 6 meses        \n",
    "#     df_mw_match['mean_wage'] = df_mw_match.loc[:, first_col: last_col].mean(axis=1)\n",
    "\n",
    "#     # Creo columna de has_job, para ver si la persona tuvo trabajo a los 6 meses\n",
    "#     df_mw_match['has_job'] = 0\n",
    "#     df_mw_match['has_job_geq_smvm'] = 0\n",
    "#     df_mw_match.loc[\n",
    "#         df_mw_match.ide_trabajador.isin(df_has_job.index), \n",
    "#         'has_job'] = 1\n",
    "#     df_mw_match.loc[\n",
    "#         df_mw_match.ide_trabajador.isin(df_has_job_smvm.index), \n",
    "#         'has_job'] = 1\n",
    "\n",
    "#     # Columna de new_job\n",
    "#     df_mw_match['new_job'] = np.where(\n",
    "#         (df_mw_match['keeps_job']==0) & (df_mw_match['has_job']==1),\n",
    "#         1,\n",
    "#         0\n",
    "#     )\n",
    "\n",
    "#     # Otras columnas\n",
    "#     df_mw_match['running_variable'] = df_mw_match['rt' + str_prev1] - smvm\n",
    "#     df_mw_match['smvm'] = smvm\n",
    "\n",
    "#     # Exporto\n",
    "#     df_mw_match.to_stata(f'df_mw_match_{shock}.dta', write_index=False)\n",
    "    \n",
    "#     # Tabla resumen\n",
    "#     out_has_job   = pd.crosstab(\n",
    "#         df_mw_match['treated'], \n",
    "#         df_mw_match['has_job']\n",
    "#         ).rename(columns={0:'no_job',1:'has_job'})\n",
    "#     out_keeps_job  = pd.crosstab(\n",
    "#         df_mw_match['treated'], \n",
    "#         df_mw_match['keeps_job']\n",
    "#         ).rename(columns={0:'lost_job',1:'keeps_job'})\n",
    "#     out_new_job  = pd.crosstab(\n",
    "#         df_mw_match['treated'], \n",
    "#         df_mw_match['new_job']\n",
    "#         ).rename(columns={0:'no_new_job',1:'new_job'})\n",
    "\n",
    "#     out = pd.concat([out_has_job, out_keeps_job, out_new_job], axis=1)\n",
    "#     out['shock'] = dt_en_t\n",
    "#     out.set_index('shock', inplace=True, append=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [col for col in df_mw_match if 'rt' in col] + ['ide_trabajador','keeps_job', 'has_job', 'new_job']\n",
    "# df_mw_match[df_mw_match.has_job == 1][cols].sort_values(by='ide_trabajador')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all[df_all.ide_trabajador == 98055740]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(rc={'figure.figsize':(8,6), 'figure.dpi':300})\n",
    "# sns.set_style(\"white\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Diferencia porcentual entre el grupo de control y el grupo de tratamiento\n",
    "\n",
    "# plot_data = []\n",
    "# for col in ['Continua', 'Despido', 'Renuncia', 'Otro Empleo']:\n",
    "\n",
    "#     df_temp = tabla[tabla['Grupo']!='Out Sample']\n",
    "\n",
    "#     df_temp[col + '_p'] = df_temp[col] / df_temp[['Continua', 'Despido', 'Renuncia', 'Otro Empleo']].sum(axis=1)\n",
    "#     df_temp = df_temp[['shock', 'Grupo',col + '_p']]\n",
    "#     df_temp = df_temp.pivot(index='shock', columns='Grupo', values=col + '_p').reset_index()\n",
    "#     df_temp[col + '_d'] = df_temp['Tratamiento'] - df_temp['Control']\n",
    "#     plot_data += [df_temp[col + '_d']]\n",
    "\n",
    "# plot_data = pd.concat(plot_data, axis=1) * 100\n",
    "# plot_data = plot_data.set_index(df_temp['shock'])\n",
    "# plot_data.to_excel('Matriz_transición.xlsx')\n",
    "\n",
    "# ## Construyo variables acumuladas:\n",
    "# plot_data = plot_data[['Despido_d', 'Otro Empleo_d', 'Renuncia_d']]\n",
    "# a = plot_data[plot_data>0].cumsum(axis=1)\n",
    "# b = plot_data[plot_data<0].cumsum(axis=1)\n",
    "# plot_data = a.fillna(0) + b.fillna(0)\n",
    "# plot_data\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1)\n",
    "# current_palette = sns.color_palette().as_hex()\n",
    "\n",
    "# bar_plot1 = sns.barplot(x=plot_data.index, y=plot_data['Renuncia_d'], label=\"Resigned\", color=current_palette[0], ax=ax, edgecolor=\"0\")\n",
    "# bar_plot2 = sns.barplot(x=plot_data.index, y=plot_data['Otro Empleo_d'], label=\"Another Job\", color=current_palette[2], ax=ax, edgecolor=\"0\")\n",
    "# bar_plot3 = sns.barplot(x=plot_data.index, y=plot_data['Despido_d'], label=\"Fired\", color=current_palette[3], ax=ax, edgecolor=\"0\")\n",
    "\n",
    "# sns.despine(bottom=True)\n",
    "# ax.legend()\n",
    "\n",
    "# ax.set_xlabel('Minimum Wage Hike')\n",
    "# ax.set_ylabel('Percentage Point Difference Between\\nTreatment and Control Groups')\n",
    "# plt.axhline(y=0, color='black')\n",
    "# plt.grid(axis='y', color='black', linestyle='-', linewidth=0.15)\n",
    "# plt.show()\n",
    "# fig.savefig('Matriz Transición diff p _ color 2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89e0abca33cf4a51fed40e84035a9d07b9b47798c9ddd5755346d755ba15d037"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
